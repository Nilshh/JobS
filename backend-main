"""
Job Aggregator Backend - FastAPI Application
Automatically searches multiple job boards and stores results
"""

from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy import create_engine, Column, String, DateTime, Integer, Boolean, Text, select
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from pydantic import BaseModel
from datetime import datetime, timedelta
from apscheduler.schedulers.background import BackgroundScheduler
import httpx
import pytz
import os
from typing import List, Optional
import logging

# ============== Configuration ==============
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://jobuser:secure_password_123@localhost:5432/job_aggregator")
DEBUG = os.getenv("DEBUG", "False") == "True"

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============== Database Setup ==============
engine = create_engine(DATABASE_URL, echo=DEBUG)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# ============== Database Models ==============
class JobSearch(Base):
    __tablename__ = "job_searches"
    
    id = Column(Integer, primary_key=True, index=True)
    title = Column(String(255), index=True)
    location = Column(String(255), index=True)
    radius = Column(Integer, default=25)
    schedule_time = Column(String(5))  # HH:MM format
    is_active = Column(Boolean, default=True)
    last_search = Column(DateTime, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)


class Job(Base):
    __tablename__ = "jobs"
    
    id = Column(Integer, primary_key=True, index=True)
    title = Column(String(255), index=True)
    company = Column(String(255))
    location = Column(String(255), index=True)
    description = Column(Text)
    url = Column(String(1000), unique=True, index=True)
    source = Column(String(100))  # arbeitnow, indeed, etc.
    salary = Column(String(255), nullable=True)
    job_type = Column(String(100), nullable=True)
    posted_date = Column(DateTime)
    found_at = Column(DateTime, default=datetime.utcnow, index=True)
    search_id = Column(Integer, nullable=True)


class User(Base):
    __tablename__ = "users"
    
    id = Column(Integer, primary_key=True, index=True)
    username = Column(String(255), unique=True, index=True)
    email = Column(String(255), unique=True, index=True)
    created_at = Column(DateTime, default=datetime.utcnow)


# Create tables
Base.metadata.create_all(bind=engine)

# ============== Pydantic Schemas ==============
class JobSearchCreate(BaseModel):
    title: str
    location: str
    radius: int = 25
    schedule_time: str  # HH:MM


class JobSearchUpdate(BaseModel):
    title: Optional[str] = None
    location: Optional[str] = None
    radius: Optional[int] = None
    schedule_time: Optional[str] = None
    is_active: Optional[bool] = None


class JobSearchResponse(BaseModel):
    id: int
    title: str
    location: str
    radius: int
    schedule_time: str
    is_active: bool
    last_search: Optional[datetime]
    created_at: datetime
    
    class Config:
        from_attributes = True


class JobResponse(BaseModel):
    id: int
    title: str
    company: str
    location: str
    url: str
    source: str
    salary: Optional[str]
    job_type: Optional[str]
    posted_date: datetime
    found_at: datetime
    
    class Config:
        from_attributes = True


# ============== Dependency ==============
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# ============== FastAPI App ==============
app = FastAPI(
    title="Job Aggregator API",
    description="Automatically search multiple job boards",
    version="1.0.0"
)

# Add CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============== Job Fetching Functions ==============
class JobAggregator:
    @staticmethod
    async def fetch_arbeitnow_jobs(job_title: str, location: str, radius: int = 25) -> List[dict]:
        """Fetch jobs from Arbeitnow API (Free, no auth required)"""
        try:
            async with httpx.AsyncClient() as client:
                url = "https://api.arbeitnow.com/api/v2/jobs"
                params = {
                    "title": job_title,
                    "location": location,
                    "page": 1
                }
                
                logger.info(f"Fetching from Arbeitnow: {params}")
                response = await client.get(url, params=params, timeout=10.0)
                response.raise_for_status()
                
                data = response.json()
                jobs = []
                
                for job in data.get("results", [])[:50]:  # Limit to 50 jobs
                    try:
                        jobs.append({
                            "title": job.get("title", "N/A"),
                            "company": job.get("company", "N/A"),
                            "location": job.get("location", location),
                            "description": job.get("description", "")[:500],
                            "url": job.get("url", ""),
                            "source": "arbeitnow",
                            "salary": job.get("salary", None),
                            "job_type": job.get("job_type", None),
                            "posted_date": datetime.fromisoformat(job.get("updated_at", datetime.utcnow().isoformat()).replace("Z", "+00:00"))
                        })
                    except Exception as e:
                        logger.warning(f"Error parsing job: {e}")
                        continue
                
                logger.info(f"Found {len(jobs)} jobs from Arbeitnow")
                return jobs
                
        except Exception as e:
            logger.error(f"Arbeitnow API error: {e}")
            return []
    
    @staticmethod
    async def fetch_all_sources(job_title: str, location: str, radius: int = 25) -> List[dict]:
        """Fetch jobs from all available sources"""
        all_jobs = []
        
        # Fetch from Arbeitnow
        arbeitnow_jobs = await JobAggregator.fetch_arbeitnow_jobs(job_title, location, radius)
        all_jobs.extend(arbeitnow_jobs)
        
        # Add more sources here in future
        # - Indeed API (requires auth)
        # - LinkedIn API (requires auth)
        # - StepStone API
        # - Monster API
        
        return all_jobs


# ============== API Endpoints - Health ==============
@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "timestamp": datetime.utcnow()}


# ============== API Endpoints - Job Searches ==============
@app.post("/api/searches", response_model=JobSearchResponse)
async def create_search(search: JobSearchCreate, db: Session = Depends(get_db)):
    """Create new job search configuration"""
    try:
        db_search = JobSearch(
            title=search.title,
            location=search.location,
            radius=search.radius,
            schedule_time=search.schedule_time,
            is_active=True
        )
        db.add(db_search)
        db.commit()
        db.refresh(db_search)
        logger.info(f"Created search: {db_search.id}")
        return db_search
    except Exception as e:
        logger.error(f"Error creating search: {e}")
        db.rollback()
        raise HTTPException(status_code=400, detail=str(e))


@app.get("/api/searches", response_model=List[JobSearchResponse])
async def list_searches(db: Session = Depends(get_db)):
    """List all job searches"""
    searches = db.query(JobSearch).all()
    return searches


@app.get("/api/searches/{search_id}", response_model=JobSearchResponse)
async def get_search(search_id: int, db: Session = Depends(get_db)):
    """Get specific search"""
    search = db.query(JobSearch).filter(JobSearch.id == search_id).first()
    if not search:
        raise HTTPException(status_code=404, detail="Search not found")
    return search


@app.put("/api/searches/{search_id}", response_model=JobSearchResponse)
async def update_search(search_id: int, update: JobSearchUpdate, db: Session = Depends(get_db)):
    """Update job search configuration"""
    search = db.query(JobSearch).filter(JobSearch.id == search_id).first()
    if not search:
        raise HTTPException(status_code=404, detail="Search not found")
    
    update_data = update.dict(exclude_unset=True)
    for key, value in update_data.items():
        setattr(search, key, value)
    
    search.updated_at = datetime.utcnow()
    db.commit()
    db.refresh(search)
    return search


@app.delete("/api/searches/{search_id}")
async def delete_search(search_id: int, db: Session = Depends(get_db)):
    """Delete job search"""
    search = db.query(JobSearch).filter(JobSearch.id == search_id).first()
    if not search:
        raise HTTPException(status_code=404, detail="Search not found")
    
    db.delete(search)
    db.commit()
    return {"message": "Search deleted"}


# ============== API Endpoints - Jobs ==============
@app.get("/api/jobs", response_model=List[JobResponse])
async def list_jobs(
    search_id: Optional[int] = None,
    days: int = 7,
    db: Session = Depends(get_db)
):
    """List jobs found in last N days"""
    cutoff_date = datetime.utcnow() - timedelta(days=days)
    query = db.query(Job).filter(Job.found_at >= cutoff_date)
    
    if search_id:
        query = query.filter(Job.search_id == search_id)
    
    jobs = query.order_by(Job.found_at.desc()).limit(1000).all()
    return jobs


@app.get("/api/jobs/search/{search_id}", response_model=List[JobResponse])
async def get_search_jobs(search_id: int, db: Session = Depends(get_db)):
    """Get jobs from specific search"""
    jobs = db.query(Job).filter(Job.search_id == search_id).order_by(Job.found_at.desc()).all()
    return jobs


@app.post("/api/jobs/trigger-search/{search_id}")
async def trigger_manual_search(
    search_id: int,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """Manually trigger job search"""
    search = db.query(JobSearch).filter(JobSearch.id == search_id).first()
    if not search:
        raise HTTPException(status_code=404, detail="Search not found")
    
    background_tasks.add_task(execute_search, search_id)
    return {"message": f"Search {search_id} triggered"}


# ============== Background Job Search ==============
async def execute_search(search_id: int):
    """Execute job search and store results"""
    db = SessionLocal()
    try:
        search = db.query(JobSearch).filter(JobSearch.id == search_id).first()
        if not search:
            logger.error(f"Search {search_id} not found")
            return
        
        logger.info(f"Executing search: {search.title} in {search.location}")
        
        # Fetch jobs from all sources
        jobs = await JobAggregator.fetch_all_sources(
            search.title,
            search.location,
            search.radius
        )
        
        # Store jobs in database
        for job_data in jobs:
            # Check if job already exists
            existing = db.query(Job).filter(Job.url == job_data["url"]).first()
            if not existing:
                db_job = Job(
                    **job_data,
                    search_id=search_id
                )
                db.add(db_job)
        
        # Update last search time
        search.last_search = datetime.utcnow()
        db.commit()
        
        logger.info(f"Search {search_id} completed. Added {len(jobs)} jobs")
        
    except Exception as e:
        logger.error(f"Error executing search {search_id}: {e}")
        db.rollback()
    finally:
        db.close()


def schedule_job_searches():
    """Setup APScheduler for automatic job searches"""
    scheduler = BackgroundScheduler()
    
    def scheduled_searches():
        db = SessionLocal()
        try:
            searches = db.query(JobSearch).filter(JobSearch.is_active == True).all()
            
            for search in searches:
                # Parse schedule time (HH:MM)
                hour, minute = map(int, search.schedule_time.split(":"))
                
                # Add job to scheduler with daily trigger at specified time
                job_id = f"search_{search.id}"
                
                if scheduler.get_job(job_id):
                    scheduler.remove_job(job_id)
                
                scheduler.add_job(
                    execute_search,
                    'cron',
                    hour=hour,
                    minute=minute,
                    id=job_id,
                    args=[search.id],
                    replace_existing=True
                )
                logger.info(f"Scheduled search {search.id} for {search.schedule_time}")
        finally:
            db.close()
    
    # Run scheduler setup on startup
    scheduled_searches()
    
    scheduler.start()
    logger.info("APScheduler started")
    return scheduler


# ============== Startup/Shutdown ==============
@app.on_event("startup")
async def startup():
    """Initialize scheduler on startup"""
    schedule_job_searches()
    logger.info("Job Aggregator started")


@app.on_event("shutdown")
async def shutdown():
    """Cleanup on shutdown"""
    logger.info("Job Aggregator shutting down")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)